{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cb6cca",
   "metadata": {},
   "source": [
    "\n",
    "# Hands-On Tutorial: Supervised Fine-Tuning with SFTTrainer\n",
    "\n",
    "This tutorial demonstrates how to perform **supervised fine-tuning (SFT)** on the `SmolLM2-135M` model using Hugging Face's `trl` library.\n",
    "\n",
    "You will learn how to:\n",
    "1. Load a pretrained language model (`SmolLM2-135M`).\n",
    "2. Format inputs as **chat conversations** using `setup_chat_format`.\n",
    "3. Run inference with the base model (before training).\n",
    "4. Load and stream a dataset from the Hugging Face Hub.\n",
    "5. Configure and run `SFTTrainer` for supervised fine-tuning.\n",
    "6. Evaluate the fine-tuned model vs. the base model.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dfef3",
   "metadata": {},
   "source": [
    "## 1) Setup & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef307af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on Colab or fresh env, uncomment:\n",
    "# !pip install -q transformers datasets trl huggingface_hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login()  # Uncomment to authenticate with your Hugging Face token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c64dc",
   "metadata": {},
   "source": [
    "## 2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f15616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307e3e8",
   "metadata": {},
   "source": [
    "## 3) Device, Model, Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\n",
    "        \"mps\"\n",
    "        if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Setup chat format (important for consistency during training + inference)\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a6960",
   "metadata": {},
   "source": [
    "## 4) Baseline Generation (Before Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=60)\n",
    "\n",
    "print(\"=== Base Model Output ===\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbab268",
   "metadata": {},
   "source": [
    "## 5) Load & Stream a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Dolly-15k dataset for instruction tuning\n",
    "streamed = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    ")\n",
    "\n",
    "# Inspect first few rows\n",
    "from itertools import islice\n",
    "\n",
    "for i, row in enumerate(islice(streamed, 3)):\n",
    "    print(f\"Row {i}:\")\n",
    "    print(\"Instruction:\", row.get(\"instruction\"))\n",
    "    print(\"Context:\", row.get(\"context\"))\n",
    "    print(\"Response:\", (row.get(\"response\") or \"\")[:120], \"...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e41456",
   "metadata": {},
   "source": [
    "### Helper: Convert Dataset Rows into Chat-Formatted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def row_to_chat_text(row):\n",
    "    system_msg = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    instr = row.get(\"instruction\") or \"\"\n",
    "    ctx = row.get(\"context\") or \"\"\n",
    "    user_msg = instr if not ctx else f\"{instr}\\n\\nContext:\\n{ctx}\"\n",
    "    assistant_msg = row.get(\"response\") or \"\"\n",
    "\n",
    "    messages = [system_msg, {\"role\": \"user\", \"content\": user_msg}]\n",
    "    if assistant_msg.strip():\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Test conversion\n",
    "streamed = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\", streaming=True)\n",
    "for i, row in enumerate(islice(streamed, 2)):\n",
    "    print(\"Formatted chat prompt:\n",
    "\", row_to_chat_text(row)[:400], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5c3b9",
   "metadata": {},
   "source": [
    "## 6) Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuning we need a non-streaming dataset (train/test splits)\n",
    "dataset = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split=\"train[:2%]\"\n",
    ")  # subset for speed\n",
    "\n",
    "\n",
    "# Map into text field for SFTTrainer\n",
    "def preprocess(example):\n",
    "    return {\"text\": row_to_chat_text(example)}\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
    "print(dataset[0][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad1db9",
   "metadata": {},
   "source": [
    "## 7) Configure SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390873df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=finetune_name,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e48812",
   "metadata": {},
   "source": [
    "## 8) Run Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498dae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341755a",
   "metadata": {},
   "source": [
    "## 9) Compare Model Outputs (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=60)\n",
    "\n",
    "print(\"=== Fine-Tuned Model Output ===\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005fe85a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "In this tutorial you:\n",
    "\n",
    "1. Ran inference with the **base model**.\n",
    "2. Loaded & **streamed a dataset** from Hugging Face Hub.\n",
    "3. Converted rows into **chat-formatted training examples**.\n",
    "4. Configured and ran **supervised fine-tuning with SFTTrainer**.\n",
    "5. Compared outputs before & after training.\n",
    "\n",
    "**Next steps:**\n",
    "- Try bigger subsets (`train[:10%]`) or full dataset for better results.\n",
    "- Experiment with different datasets (e.g., `HuggingFaceTB/smoltalk`, `OpenAssistant/oasst1`).\n",
    "- Push your fine-tuned model to the Hugging Face Hub for reuse!\n",
    "\n",
    "Happy fine-tuning ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
