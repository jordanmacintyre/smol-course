{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8abce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1460cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\"  # example; use yours\n",
    "\n",
    "# Use bf16 if available, else fp16\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "max_memory = {\n",
    "    0: \"1GiB\",  # keep only ~1GiB of the model on GPU0\n",
    "    1: \"7GiB\",  # rest of the model goes here\n",
    "    \"cpu\": \"24GiB\",  # optional spillover/offload safety\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",  # let HF place layers under the caps\n",
    "    max_memory=max_memory,\n",
    "    low_cpu_mem_usage=True,  # streams weights in\n",
    "    offload_folder=\"./offload\",  # only used if it needs to spill to CPU\n",
    ")\n",
    "\n",
    "# (Training tip) disable cache + enable checkpointing to reduce activations\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Setup for the model specific chat format\n",
    "if not getattr(tokenizer, \"chat_template\", None):\n",
    "    model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# When you create batches, put inputs on the device that holds the FIRST layer.\n",
    "# Auto placement usually puts embeddings & early blocks on the *smaller* device.\n",
    "first_device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d991632",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"trl-lib/Capybara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9fa534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp\n",
    "current_time = datetime.now()\n",
    "\n",
    "# Create a readable timestamp\n",
    "formatted_time = current_time.strftime(\"%b-%d-%Y-%H-%M-%S\")\n",
    "\n",
    "# Adjust the model\n",
    "fine_tuned_model_name = f'{model_id.replace(\"/\", \"--\")}-ft'\n",
    "\n",
    "# Model assets output folder\n",
    "model_output_folder = Path.cwd().joinpath(f\"models/{fine_tuned_model_name}\")\n",
    "if not model_output_folder.exists():\n",
    "    model_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# SFTrainer configuration\n",
    "sft_config = SFTConfig(\n",
    "    # Output directory for model assets\n",
    "    output_dir=model_output_folder,\n",
    "    # Hyperparameter : Controls maximum number of steps to be executed\n",
    "    # Maximum number of gradient update steps during training.\n",
    "    max_steps=200,\n",
    "    # Common starting point for fine-tuning\n",
    "    # The initial learning rate for the optimizer.\n",
    "    learning_rate=5e-5,\n",
    "    # Set according to your GPU memory capacity\n",
    "    # Number of training samples per device in each batch. Smaller values help fit large models into memory-constrained GPUs.\n",
    "    per_device_train_batch_size=1,\n",
    "    # Simulate batch size = 8\n",
    "    gradient_accumulation_steps=8,\n",
    "    # Frequency of logging training metrics\n",
    "    # Logs metrics (e.g., loss) every 10 steps during training.\n",
    "    logging_steps=10,\n",
    "    # Frequency of saving model checkpoints\n",
    "    # Saves model checkpoints every 100 steps. In case of failure, loss or work will be limited to a maximum of 100 steps\n",
    "    save_steps=100,\n",
    "    # Evaluate the model at regular intervals\n",
    "    eval_strategy=\"steps\",\n",
    "    # Frequency of evaluation\n",
    "    # Run the model evaluation after every 50 steps\n",
    "    eval_steps=50,\n",
    "    # Use MPS for mixed precision training\n",
    "    use_mps_device=(True if first_device == \"mps\" else False),\n",
    "    # Set a unique name for your model - used for HuggingFace hub\n",
    "    hub_model_id=fine_tuned_model_name,\n",
    "    max_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3025d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    # The language model being fine-tuned.\n",
    "    model=model,\n",
    "    # Passes the fine-tuning configuration defined above\n",
    "    args=sft_config,\n",
    "    # Training dataset\n",
    "    train_dataset=ds[\"train\"],\n",
    "    # Evaluation dataset\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    # Tokenizer used\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb7eee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 03:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.747800</td>\n",
       "      <td>1.880180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.735200</td>\n",
       "      <td>1.842487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.779300</td>\n",
       "      <td>1.826471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.831600</td>\n",
       "      <td>1.823102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.8082774925231933, metrics={'train_runtime': 229.0441, 'train_samples_per_second': 6.986, 'train_steps_per_second': 0.873, 'total_flos': 523354621870080.0, 'train_loss': 1.8082774925231933})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fab7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# trainer.save_model(f\"./{fine_tuned_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
