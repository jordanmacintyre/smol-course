{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8abce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be set before importing torch/transformers\n",
    "import os\n",
    "\n",
    "# If reserved unallocated memory is large\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
    "\n",
    "# (optional) avoid the fork/threads warning and nested parallelism\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# Ensures that only 1 GPU is visible to torch/accelerate/transformers/trl\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# logging.set_verbosity_error()\n",
    "\n",
    "OUTPUT_DIR = Path.cwd().joinpath(\"ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1460cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn-Implementation: eager\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 0) Setup\n",
    "# ------------------------------\n",
    "# model_id = \"google/gemma-3-270m\"\n",
    "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "# Use bf16 if available, else fp16\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Many Qwen tokenizers have no explicit pad_token; for training we usually\n",
    "# set pad_token = eos_token so padding is benign for loss.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_memory = {\n",
    "    0: \"8GiB\",  # keep part of the model on GPU0\n",
    "    # 1: \"8GiB\",  # rest of the model goes here\n",
    "    # \"cpu\": \"24GiB\",  # optional spillover/offload safety\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    # First, try to use community vLLM Flash-Attn 3\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",  # let HF place layers under the caps\n",
    "        max_memory=max_memory,\n",
    "        attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "        offload_folder=\"./offload\",  # only used if it needs to spill to CPU\n",
    "    )\n",
    "    # Fallback to Flash-Attn 2\n",
    "except:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",  # let HF place layers under the caps\n",
    "        max_memory=max_memory,\n",
    "        attn_implementation=\"eager\",  # flash_attention_2\n",
    "        offload_folder=\"./offload\",  # only used if it needs to spill to CPU\n",
    "    )\n",
    "\n",
    "print(\"Attn-Implementation:\", model.config._attn_implementation)\n",
    "\n",
    "# (Training tip) disable cache + enable checkpointing to reduce activations\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Setup for the model specific chat format\n",
    "if not getattr(tokenizer, \"chat_template\", None):\n",
    "    model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# When you create batches, put inputs on the device that holds the FIRST layer.\n",
    "# Auto placement usually puts embeddings & early blocks on the *smaller* device.\n",
    "first_device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1) Load the dataset\n",
    "#    trl-lib/tldr has columns: \"prompt\" (the post) and \"completion\" (TL;DR)\n",
    "# ------------------------------\n",
    "dataset_name = \"trl-lib/tldr\"\n",
    "\n",
    "raw_train = load_dataset(dataset_name, split=\"train[:5%]\")\n",
    "raw_val = load_dataset(dataset_name, split=\"test[:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eb3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2) Formatting: produce a single string per row with a clear response boundary\n",
    "#\n",
    "# Important details:\n",
    "# - The boundary string *must* match what you pass to response_template below\n",
    "#   (including spaces/punctuation/case).\n",
    "# - Because Reddit posts can be long, we cap the prompt to keep the completion\n",
    "#   inside the max_seq_length budget. Two versions are shown:\n",
    "#     (A) simple char cap (very fast, approximate)\n",
    "#     (B) token-budgeted cap (more precise, a little slower)\n",
    "# Pick one and comment out the other.\n",
    "# ------------------------------\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "BOUNDARY = \"TL;DR: \"  # <-- Will be used as response_template\n",
    "\n",
    "\n",
    "# (A) Simple, fast char-cap (good enough for many runs)\n",
    "def format_pc_char_cap(example, max_prompt_chars=MAX_LENGTH):\n",
    "    prompt = example[\"prompt\"]\n",
    "    # Trim the prompt aggressively so the summary isn't truncated\n",
    "    if len(prompt) > max_prompt_chars:\n",
    "        prompt = prompt[:max_prompt_chars] + \"â€¦\"\n",
    "    text = (\n",
    "        \"Summarize the post below in a single concise TL;DR.\\n\\n\"\n",
    "        f\"{prompt}\\n\\n{BOUNDARY}{example['completion']}\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "# (B) Token-budgeted cap (keeps the *end* with completion intact more reliably)\n",
    "def format_pc_token_cap(example, max_len=MAX_LENGTH, reserve_for_completion=128):\n",
    "    # Tokenize completion to estimate space needed for summary + boundary + EOS\n",
    "    comp_ids = tokenizer(example[\"completion\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    # reserve a bit more for the boundary + eos\n",
    "    reserve = min(max_len // 3, reserve_for_completion) + 16\n",
    "\n",
    "    # Budget for the prompt = total - reserve\n",
    "    prompt_budget = max_len - (len(comp_ids) + reserve)\n",
    "    prompt_budget = max(prompt_budget, 32)  # still keep some prompt\n",
    "\n",
    "    # Take only the first `prompt_budget` tokens of the prompt\n",
    "    prompt_ids = tokenizer(example[\"prompt\"], add_special_tokens=False)[\"input_ids\"][\n",
    "        :prompt_budget\n",
    "    ]\n",
    "    prompt_trimmed = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "\n",
    "    text = (\n",
    "        \"Summarize the post below in a single concise TL;DR.\\n\\n\"\n",
    "        f\"{prompt_trimmed}\\n\\n{BOUNDARY}{example['completion']}{tokenizer.eos_token}\"\n",
    "    )\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8899f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1e865111154d25bbd3c0bc6d3af12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting train:   0%|          | 0/5836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose ONE formatter:\n",
    "use_token_budget = True\n",
    "fmt_fn = format_pc_token_cap if use_token_budget else format_pc_char_cap\n",
    "\n",
    "train = raw_train.map(\n",
    "    fmt_fn, remove_columns=raw_train.column_names, desc=\"Formatting train\"\n",
    ")\n",
    "val = raw_val.map(fmt_fn, remove_columns=raw_val.column_names, desc=\"Formatting val\")\n",
    "\n",
    "# Optional: drop everything except the 'text' column (keeps memory small)\n",
    "train = train.remove_columns([c for c in train.column_names if c != \"text\"])\n",
    "val = val.remove_columns([c for c in val.column_names if c != \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1305e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3) Trainer config\n",
    "# Notes:\n",
    "# - packing=True concatenates multiple short samples together to reach the max\n",
    "#   length. This reduces padding and usually increases throughput.\n",
    "# - bf16 is fast on Ampere+ (RTX 30xx); if unsupported, set bf16=False and fp16=True.\n",
    "# - Adjust max_length based on your VRAM and throughput goals.\n",
    "# ------------------------------\n",
    "ft_filename = \"SmoLM2-135M-tldr-sft\"\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR.joinpath(ft_filename),\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    packing=False,\n",
    "    bf16=True,  # assumes Ampere; set to False if needed\n",
    "    # fp16=True,  # mutual exclusive with bf16\n",
    "    tf32=True,\n",
    "    gradient_checkpointing=False,  # lowers memory, costs time; toggle as needed\n",
    "    # gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    # logging_steps=25,\n",
    "    # save_steps=100,\n",
    "    # eval_steps=25,\n",
    "    use_liger_kernel=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch_fused\",  # if your torch supports it; else \"adamw_torch\"\n",
    "    # optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    completion_only_loss=True,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_num_workers=8,  # try 4â€“8, depends on CPU\n",
    "    dataloader_persistent_workers=True,\n",
    "    dataloader_prefetch_factor=4,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    report_to=[],  # disable W&B by default\n",
    ")\n",
    "\n",
    "# If using packing, the attention implementation should be set to\n",
    "# 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens\n",
    "# batches into a single sequence, and Flash Attention is the only known attention\n",
    "# mechanisms that reliably support this. Using other implementations may lead to\n",
    "# cross-contamination between batches. To avoid this, either disable packing by setting\n",
    "# `packing=False`, or set `attn_implementation='flash_attention_2'` or\n",
    "# `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c7e6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0457f8f3239c452e972376c617079848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/5836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b327a9840ca54ef58c21780810c9e265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58d5dadb45f45959abd55870696b282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/5836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 4) Trainer\n",
    "# If youâ€™re memory-constrained, you can add LoRA or 4-bit later.\n",
    "# For clarity, this example fine-tunes full weights in bf16/fp16.\n",
    "# ------------------------------\n",
    "train = train.with_format(\"torch\", columns=[\"text\"])\n",
    "val = val.with_format(\"torch\", columns=[\"text\"])\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    args=cfg,\n",
    ")\n",
    "\n",
    "# Required for training with checkpointing (turns off KV cache during train)\n",
    "trainer.model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00cc4034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='182' max='182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [182/182 03:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.615700</td>\n",
       "      <td>2.559173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=182, training_loss=2.6157454605940935, metrics={'train_runtime': 223.1241, 'train_samples_per_second': 26.156, 'train_steps_per_second': 0.816, 'total_flos': 1454780038053888.0, 'train_loss': 2.6157454605940935})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72fab7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(OUTPUT_DIR.joinpath(ft_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b86ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE/AFTER QUICK CHECK â€” works for full-FT or LoRA outputs\n",
    "FT_DIR = OUTPUT_DIR.joinpath(ft_filename)  # your SFTConfig.output_dir\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, use_fast=True, trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def load_base():\n",
    "    return (\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, torch_dtype=DTYPE, trust_remote_code=True\n",
    "        )\n",
    "        .to(device)\n",
    "        .eval()\n",
    "    )\n",
    "\n",
    "\n",
    "def load_finetuned():\n",
    "    # If LoRA/PEFT adapters exist, attach them to the base; else load full FT weights.\n",
    "    if os.path.isfile(os.path.join(FT_DIR, \"adapter_config.json\")):\n",
    "        from peft import PeftModel\n",
    "\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, torch_dtype=DTYPE, trust_remote_code=True\n",
    "        ).to(device)\n",
    "        ft = PeftModel.from_pretrained(base, FT_DIR).to(device)\n",
    "        ft.eval()\n",
    "        return ft\n",
    "    else:\n",
    "        return (\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                FT_DIR, torch_dtype=DTYPE, trust_remote_code=True\n",
    "            )\n",
    "            .to(device)\n",
    "            .eval()\n",
    "        )\n",
    "\n",
    "\n",
    "def generate(model, prompt, max_new_tokens=64, do_sample=False):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=0.8 if do_sample else None,\n",
    "            top_p=0.95 if do_sample else None,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_type_id,\n",
    "        )\n",
    "    gen = tokenizer.decode(\n",
    "        out[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "    return gen.strip()\n",
    "\n",
    "\n",
    "ds = load_dataset(\"trl-lib/tldr\", split=\"validation\")\n",
    "prompt_text = f\"Summarize the post below in a single TL;DR.\\n\\n{ds[42]['prompt']}\"\n",
    "\n",
    "# ---- Run BEFORE/AFTER ----\n",
    "base_model = load_base()\n",
    "ft_model = load_finetuned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e976c84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Text: Summarize the post below in a single TL;DR.\n",
      "\n",
      "SUBREDDIT: r/jobs\n",
      "\n",
      "TITLE: Cold applying for a marketing position in a small local company by attaching a proposal for their business website. Feasible idea?\n",
      "\n",
      "POST: Hello /r/jobs, I graduated a few months ago and had no luck so far to get a job in marketing/sales. \n",
      "\n",
      "There's a small local company (perhaps 30 employees) but they are actually pretty successful in what they're doing (known worldwide). I checked their website and it's awful. Looks like a website from the early 2000's. So I guess they are not pretty good in (online-)marketing. \n",
      "\n",
      "I would like to do a cold application (not sure if they are looking for a marketing guy) but I had no luck with this kind of application in the past. That's why I thought I try something different. I have good skills in photoshop, indesign and illustrator. As a teenager I also built websites using HTML, so I thought I build a dummy website fitted to their company and attach some screenshots to my application.\n",
      "\n",
      "What do you think? I this a feasible idea or will they be offended?\n",
      "\n",
      "Thank you very much in advance.\n",
      "\n",
      "TL;DR:\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt Text:\", prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a08d3244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BEFORE (base) ---\n",
      "I'm not sure if they are looking for a marketing guy or not.\n",
      "\n",
      "I'm not sure if they are looking for a marketing guy or not.\n",
      "\n",
      "I'm not sure if they are looking for a marketing guy or not.\n",
      "\n",
      "I'm not sure if they are looking for a marketing guy or\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- BEFORE (base) ---\")\n",
    "print(generate(base_model, prompt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1321da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AFTER (fine-tuned) ---\n",
      "TL;DR:  I want to do a cold application for a small local company by attaching a proposal for their business website. I don't know if they are looking for a marketing guy or not. TL;DR:  I don't know if they are looking for a marketing guy or not. TL\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- AFTER (fine-tuned) ---\")\n",
    "print(generate(ft_model, prompt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cdae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
