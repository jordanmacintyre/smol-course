{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8abce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be set before importing torch/transformers\n",
    "import os\n",
    "\n",
    "# If reserved unallocated memory is large\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
    "\n",
    "# Ensures that only 1 GPU is visible to torch/accelerate/transformers/trl\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1460cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn-Implementation: flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 0) Setup\n",
    "# ------------------------------\n",
    "model_id = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "# Use bf16 if available, else fp16\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Many Qwen tokenizers have no explicit pad_token; for training we usually\n",
    "# set pad_token = eos_token so padding is benign for loss.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_memory = {\n",
    "    0: \"8GiB\",  # keep part of the model on GPU0\n",
    "    # 1: \"8GiB\",  # rest of the model goes here\n",
    "    # \"cpu\": \"24GiB\",  # optional spillover/offload safety\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    # First, try to use community vLLM Flash-Attn 3\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",  # let HF place layers under the caps\n",
    "        max_memory=max_memory,\n",
    "        attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    "        offload_folder=\"./offload\",  # only used if it needs to spill to CPU\n",
    "    )\n",
    "    # Fallback to Flash-Attn 2\n",
    "except:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",  # let HF place layers under the caps\n",
    "        max_memory=max_memory,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        offload_folder=\"./offload\",  # only used if it needs to spill to CPU\n",
    "    )\n",
    "\n",
    "print(\"Attn-Implementation:\", model.config._attn_implementation)\n",
    "\n",
    "# (Training tip) disable cache + enable checkpointing to reduce activations\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Setup for the model specific chat format\n",
    "if not getattr(tokenizer, \"chat_template\", None):\n",
    "    model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# When you create batches, put inputs on the device that holds the FIRST layer.\n",
    "# Auto placement usually puts embeddings & early blocks on the *smaller* device.\n",
    "first_device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d991632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1) Load the dataset\n",
    "#    trl-lib/tldr has columns: \"prompt\" (the post) and \"completion\" (TL;DR)\n",
    "# ------------------------------\n",
    "raw_train = load_dataset(\"trl-lib/tldr\", split=\"train[:10%]\")\n",
    "raw_val = load_dataset(\"trl-lib/tldr\", split=\"test[:10%]\")  # optional eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eb3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2) Formatting: produce a single string per row with a clear response boundary\n",
    "#\n",
    "# Important details:\n",
    "# - The boundary string *must* match what you pass to response_template below\n",
    "#   (including spaces/punctuation/case).\n",
    "# - Because Reddit posts can be long, we cap the prompt to keep the completion\n",
    "#   inside the max_seq_length budget. Two versions are shown:\n",
    "#     (A) simple char cap (very fast, approximate)\n",
    "#     (B) token-budgeted cap (more precise, a little slower)\n",
    "# Pick one and comment out the other.\n",
    "# ------------------------------\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "BOUNDARY = \"TL;DR: \"  # <-- Will be used as response_template\n",
    "\n",
    "\n",
    "# (A) Simple, fast char-cap (good enough for many runs)\n",
    "def format_pc_char_cap(example, max_prompt_chars=MAX_LENGTH):\n",
    "    prompt = example[\"prompt\"]\n",
    "    # Trim the prompt aggressively so the summary isn't truncated\n",
    "    if len(prompt) > max_prompt_chars:\n",
    "        prompt = prompt[:max_prompt_chars] + \"…\"\n",
    "    text = (\n",
    "        \"Summarize the post below in a single concise TL;DR.\\n\\n\"\n",
    "        f\"{prompt}\\n\\n{BOUNDARY}{example['completion']}\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "# (B) Token-budgeted cap (keeps the *end* with completion intact more reliably)\n",
    "def format_pc_token_cap(example, max_len=MAX_LENGTH, reserve_for_completion=128):\n",
    "    # Tokenize completion to estimate space needed for summary + boundary + EOS\n",
    "    comp_ids = tokenizer(example[\"completion\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    # reserve a bit more for the boundary + eos\n",
    "    reserve = min(max_len // 3, reserve_for_completion) + 16\n",
    "\n",
    "    # Budget for the prompt = total - reserve\n",
    "    prompt_budget = max_len - (len(comp_ids) + reserve)\n",
    "    prompt_budget = max(prompt_budget, 32)  # still keep some prompt\n",
    "\n",
    "    # Take only the first `prompt_budget` tokens of the prompt\n",
    "    prompt_ids = tokenizer(example[\"prompt\"], add_special_tokens=False)[\"input_ids\"][\n",
    "        :prompt_budget\n",
    "    ]\n",
    "    prompt_trimmed = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "\n",
    "    text = (\n",
    "        \"Summarize the post below in a single concise TL;DR.\\n\\n\"\n",
    "        f\"{prompt_trimmed}\\n\\n{BOUNDARY}{example['completion']}\"\n",
    "    )\n",
    "    return {\"text\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8899f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4337ae2a6a4ad0a305bb8183cd1a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting train:   0%|          | 0/11672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose ONE formatter:\n",
    "use_token_budget = False\n",
    "fmt_fn = format_pc_token_cap if use_token_budget else format_pc_char_cap\n",
    "\n",
    "train = raw_train.map(\n",
    "    fmt_fn, remove_columns=raw_train.column_names, desc=\"Formatting train\"\n",
    ")\n",
    "val = raw_val.map(fmt_fn, remove_columns=raw_val.column_names, desc=\"Formatting val\")\n",
    "\n",
    "# Optional: drop everything except the 'text' column (keeps memory small)\n",
    "train = train.remove_columns([c for c in train.column_names if c != \"text\"])\n",
    "val = val.remove_columns([c for c in val.column_names if c != \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1305e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3) Trainer config\n",
    "# Notes:\n",
    "# - packing=True concatenates multiple short samples together to reach the max\n",
    "#   length. This reduces padding and usually increases throughput.\n",
    "# - bf16 is fast on Ampere+ (RTX 30xx); if unsupported, set bf16=False and fp16=True.\n",
    "# - Adjust max_length based on your VRAM and throughput goals.\n",
    "# ------------------------------\n",
    "output_dir = \"./models/SmolLM2-135M-tldr-sft\"\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    packing=True,\n",
    "    bf16=True,  # assumes Ampere; set to False if needed\n",
    "    fp16=False,  # mutual exclusive with bf16\n",
    "    gradient_checkpointing=False,  # lowers memory, costs time; toggle as needed\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # simulate global BS=16\n",
    "    num_train_epochs=1,\n",
    "    # logging_steps=25,\n",
    "    # save_steps=100,\n",
    "    # eval_steps=25,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    # optim=\"adamw_torch_fused\",  # if your torch supports it; else \"adamw_torch\"\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=1e-4,\n",
    "    completion_only_loss=True,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_persistent_workers=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=[],  # disable W&B by default\n",
    ")\n",
    "\n",
    "# If using packing, the attention implementation should be set to\n",
    "# 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens\n",
    "# batches into a single sequence, and Flash Attention is the only known attention\n",
    "# mechanisms that reliably support this. Using other implementations may lead to\n",
    "# cross-contamination between batches. To avoid this, either disable packing by setting\n",
    "# `packing=False`, or set `attn_implementation='flash_attention_2'` or\n",
    "# `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c7e6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd8887189f74a8e9c24350add8ae72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/11672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db71836c1fd547bfbe22aa09a3dd4308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/11672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3cdae99a46440193f4f1f644bae985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/11672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 4) Trainer\n",
    "# If you’re memory-constrained, you can add LoRA or 4-bit later.\n",
    "# For clarity, this example fine-tunes full weights in bf16/fp16.\n",
    "# ------------------------------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    args=cfg,\n",
    ")\n",
    "\n",
    "# Required for training with checkpointing (turns off KV cache during train)\n",
    "trainer.model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00cc4034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='363' max='363' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [363/363 03:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.336000</td>\n",
       "      <td>2.281568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=363, training_loss=2.3360256058453857, metrics={'train_runtime': 221.7856, 'train_samples_per_second': 26.124, 'train_steps_per_second': 1.637, 'total_flos': 1442423690762112.0, 'train_loss': 2.3360256058453857})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72fab7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b86ca6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Text: Summarize the following post in one short TL;DR:\n",
      "\n",
      "I adopted a cat last week. She's shy but already using the litter box and eating well. How can I help her adjust faster?\n",
      "\n",
      "TL;DR: \n",
      "\n",
      "--- BEFORE (base) ---\n",
      "1. Don't be a cat person. 2. Don't be a cat person. 3. Don't be a cat person. 4.\n",
      "\n",
      "--- AFTER (fine-tuned) ---\n",
      "I adopted a cat, she's shy, and I'm trying to help her adjust. How can I help her?SummitSummitSummitSum\n"
     ]
    }
   ],
   "source": [
    "# BEFORE/AFTER QUICK CHECK — works for full-FT or LoRA outputs\n",
    "FT_DIR = output_dir  # your SFTConfig.output_dir\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "\n",
    "def load_base():\n",
    "    return (\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, torch_dtype=DTYPE, trust_remote_code=True\n",
    "        )\n",
    "        .to(device)\n",
    "        .eval()\n",
    "    )\n",
    "\n",
    "\n",
    "def load_finetuned():\n",
    "    # If LoRA/PEFT adapters exist, attach them to the base; else load full FT weights.\n",
    "    if os.path.isfile(os.path.join(FT_DIR, \"adapter_config.json\")):\n",
    "        from peft import PeftModel\n",
    "\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, torch_dtype=DTYPE, trust_remote_code=True\n",
    "        ).to(device)\n",
    "        ft = PeftModel.from_pretrained(base, FT_DIR).to(device)\n",
    "        ft.eval()\n",
    "        return ft\n",
    "    else:\n",
    "        return (\n",
    "            AutoModelForCausalLM.from_pretrained(\n",
    "                FT_DIR, torch_dtype=DTYPE, trust_remote_code=True\n",
    "            )\n",
    "            .to(device)\n",
    "            .eval()\n",
    "        )\n",
    "\n",
    "\n",
    "def generate(model, prompt, max_new_tokens=32, do_sample=False):\n",
    "    model.eval()\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=0.8 if do_sample else None,\n",
    "            top_p=0.95 if do_sample else None,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    gen = tok.decode(out[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n",
    "    return gen.strip()\n",
    "\n",
    "\n",
    "# ---- Choose a quick prompt ----\n",
    "# (A) Use your own text:\n",
    "prompt_text = \"Summarize the following post in one short TL;DR:\\n\\nI adopted a cat last week. She's shy but already using the litter box and eating well. How can I help her adjust faster?\\n\\nTL;DR: \"\n",
    "\n",
    "# (B) Or pull one sample from TLDR validation (uncomment):\n",
    "# ds = load_dataset(\"trl-lib/tldr\", split=\"validation\")\n",
    "# prompt_text = f\"Summarize the post below in a single TL;DR.\\n\\n{ds[0]['prompt']}\\n\\nTL;DR: \"\n",
    "\n",
    "# ---- Run BEFORE/AFTER ----\n",
    "base_model = load_base()\n",
    "ft_model = load_finetuned()\n",
    "\n",
    "print(\"Prompt Text:\", prompt_text)\n",
    "\n",
    "print(\"\\n--- BEFORE (base) ---\")\n",
    "print(generate(base_model, prompt_text))\n",
    "\n",
    "print(\"\\n--- AFTER (fine-tuned) ---\")\n",
    "print(generate(ft_model, prompt_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976c84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
