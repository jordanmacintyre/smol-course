{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf31846",
   "metadata": {},
   "source": [
    "# Hands-On Tutorial: Chat Templates with SmolLM2 **(+ Streaming Datasets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10c55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import setup_chat_format\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf208066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4250f209014166b30c0835c2b2bd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138a10d1c4174c40beefa737da725719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fd887dfeb54f88b49c568a1a7ed1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7b844914e840d295d32b377ebc39a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6410ff53123b4ee4b1ec8757d47e806b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99ee020c24745738849b5a4dc6b9a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033803b4a8884327b63798471fd1f270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\n",
    "        \"mps\"\n",
    "        if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef4fa60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{ bos_token }}\\n{%- if messages[0][\\'role\\'] == \\'system\\' -%}\\n    {%- if messages[0][\\'content\\'] is string -%}\\n        {%- set first_user_prefix = messages[0][\\'content\\'] + \\'\\n\\n\\' -%}\\n    {%- else -%}\\n        {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\n\\n\\' -%}\\n    {%- endif -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \"\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\\n    {%- endif -%}\\n    {%- if (message[\\'role\\'] == \\'assistant\\') -%}\\n        {%- set role = \"model\" -%}\\n    {%- else -%}\\n        {%- set role = message[\\'role\\'] -%}\\n    {%- endif -%}\\n    {{ \\'<start_of_turn>\\' + role + \\'\\n\\' + (first_user_prefix if loop.first else \"\") }}\\n    {%- if message[\\'content\\'] is string -%}\\n        {{ message[\\'content\\'] | trim }}\\n    {%- elif message[\\'content\\'] is iterable -%}\\n        {%- for item in message[\\'content\\'] -%}\\n            {%- if item[\\'type\\'] == \\'image\\' -%}\\n                {{ \\'<start_of_image>\\' }}\\n            {%- elif item[\\'type\\'] == \\'text\\' -%}\\n                {{ item[\\'text\\'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\"Invalid content type\") }}\\n    {%- endif -%}\\n    {{ \\'<end_of_turn>\\n\\' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{\\'<start_of_turn>model\\n\\'}}\\n{%- endif -%}\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(tokenizer, \"chat_template\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66b193a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before setup:\n",
      " {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n",
      "After setup:\n",
      " {{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Template *before* setup\n",
    "print(\"Before setup:\\n\", tokenizer.chat_template)\n",
    "\n",
    "# Enable chat template support (adds/aligns special tokens and template behavior)\n",
    "# If this is not configured, tokenizer.apply_chat_template() will break\n",
    "if not getattr(tokenizer, \"chat_template\", None):\n",
    "    model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Template *after* setup\n",
    "print(\"After setup:\\n\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85853dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt preview:\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant that explains things clearly.\n",
      "\n",
      "Explain what chat templates are in relation to large language models (LLMs).<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain what chat templates are in relation to large language models (LLMs).\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that explains things clearly.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(\"Formatted prompt preview:\\n\")\n",
    "print(formatted_prompt[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f275acb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Output (with template) ===\n",
      "\n",
      "user\n",
      "You are a helpful assistant that explains things clearly.\n",
      "\n",
      "Explain what chat templates are in relation to large language models (LLMs).\n",
      "Okay, let's dive into chat templates.  I'm going to explain it in a way that's easy to understand.\n",
      "\n",
      "**What are Chat Templates?**\n",
      "\n",
      "Imagine you're talking to a very smart, but sometimes a little literal, assistant. Chat templates are like a pre-written set of phrases and instructions that you give to an LLM (like ChatGPT, Bard, etc.) to guide its responses. \n",
      "\n",
      "Think of it like this: you're giving the LLM a recipe. You tell it *what* you want, but you don't\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9\n",
    "    )\n",
    "\n",
    "print(\"=== Model Output (with template) ===\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71847db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Output (without template) ===\n",
      "\n",
      "Explain what chat templates are in relation to large language models (LLMs).\n",
      "\n",
      "**Chat Templates**\n",
      "\n",
      "Chat templates are a way to structure and streamline conversations with large language models (LLMs) like GPT-3, Bard, and others. They're essentially pre-defined sets of prompts and instructions that guide the LLM towards a specific task or output.  Think of them as a blueprint for the conversation.\n",
      "\n",
      "Here's a breakdown of key aspects:\n",
      "\n",
      "* **Structure:** They define the *flow* of a conversation.  They dictate what the LLM should respond to and how it should respond.\n",
      "* **Specificity:** They're highly targeted\n"
     ]
    }
   ],
   "source": [
    "raw_prompt = prompt\n",
    "raw_inputs = tokenizer(raw_prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    raw_outputs = model.generate(\n",
    "        **raw_inputs, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9\n",
    "    )\n",
    "\n",
    "print(\"=== Model Output (without template) ===\\n\")\n",
    "print(tokenizer.decode(raw_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d03f4b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Turn Output ===\n",
      "\n",
      "user\n",
      "You are a witty assistant that answers concisely.\n",
      "\n",
      "What is quantum computing?\n",
      "model\n",
      "Quantum computing uses quantum bits (qubits) that can be in multiple states at once.\n",
      "user\n",
      "Explain it like I'm 5.\n",
      " \n",
      "Imagine you have a light switch. It can be either on OR off, right? \n",
      "\n",
      "Quantum computers are like having a light switch that can be *both* on and off *at the same time*!  It's a bit weird, but it lets them solve really, really complicated problems much faster than regular computers. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a witty assistant that answers concisely.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is quantum computing?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Quantum computing uses quantum bits (qubits) that can be in multiple states at once.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Explain it like I'm 5.\"},\n",
    "]\n",
    "\n",
    "mt_prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "mt_inputs = tokenizer(mt_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mt_outputs = model.generate(\n",
    "        **mt_inputs, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.9\n",
    "    )\n",
    "\n",
    "print(\"=== Multi-Turn Output ===\\n\")\n",
    "print(tokenizer.decode(mt_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2bd0e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] keys: ['instruction', 'context', 'response', 'category']\n",
      "instruction: When did Virgin Australia start operating?\n",
      "context: Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
      "response: Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. ...\n",
      "--------------------------------------------------------------------------------\n",
      "[1] keys: ['instruction', 'context', 'response', 'category']\n",
      "instruction: Which is a species of fish? Tope or Rope\n",
      "context: \n",
      "response: Tope ...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] keys: ['instruction', 'context', 'response', 'category']\n",
      "instruction: Why can camels survive for long without water?\n",
      "context: \n",
      "response: Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time. ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# Stream a dataset (no full download). You can swap for other datasets.\n",
    "streamed = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    ")\n",
    "\n",
    "# Peek a few rows to understand the schema\n",
    "for i, row in enumerate(islice(streamed, 3)):\n",
    "    print(f\"[{i}] keys:\", list(row.keys()))\n",
    "    print(\"instruction:\", row.get(\"instruction\"))\n",
    "    print(\"context:\", row.get(\"context\"))\n",
    "    print(\"response:\", (row.get(\"response\") or \"\")[:120], \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f50b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Sample 0 (formatted preview) ----\n",
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant that follows instructions carefully.\n",
      "\n",
      "When did Virgin Australia start operating?\n",
      "\n",
      "Context:\n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single  ...\n",
      "\n",
      "---- Sample 1 (formatted preview) ----\n",
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant that follows instructions carefully.\n",
      "\n",
      "Which is a species of fish? Tope or Rope<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Tope<end_of_turn>\n",
      " ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def row_to_messages(row):\n",
    "    sys_msg = \"You are a helpful assistant that follows instructions carefully.\"\n",
    "    instr = row.get(\"instruction\") or \"\"\n",
    "    ctx = row.get(\"context\") or \"\"\n",
    "    user_msg = instr if not ctx else f\"{instr}\\n\\nContext:\\n{ctx}\"\n",
    "    asst_msg = row.get(\"response\") or \"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "\n",
    "    # Include gold answer for inspection; for inference we usually omit it\n",
    "    if asst_msg.strip():\n",
    "        messages.append({\"role\": \"assistant\", \"content\": asst_msg})\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "# Smoke test on a couple of samples\n",
    "streamed = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    ")\n",
    "for i, row in enumerate(islice(streamed, 2)):\n",
    "    msgs = row_to_messages(row)\n",
    "    formatted = tokenizer.apply_chat_template(msgs, tokenize=False)\n",
    "    print(f\"---- Sample {i} (formatted preview) ----\\n{formatted[:400]} ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec763ab",
   "metadata": {},
   "source": [
    "### On-the-Fly Tokenization & Inference Over the Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27461fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Streamed Sample #0 =====\n",
      "User prompt:\n",
      " When did Virgin Australia start operating?\n",
      "\n",
      "Context:\n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 ci\n",
      "\n",
      "### Model response ###:\n",
      " user\n",
      "You are a helpful assistant that follows instructions carefully.\n",
      "\n",
      "When did Virgin Australia start operating?\n",
      "\n",
      "Context:\n",
      "Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
      "Here's your answer:\n",
      "\n",
      "Virgin Australia started operating on August 31, 2000.\n",
      "\n",
      "\n",
      "===== Streamed Sample #1 =====\n",
      "User prompt:\n",
      " Which is a species of fish? Tope or Rope\n",
      "\n",
      "### Model response ###:\n",
      " user\n",
      "You are a helpful assistant that follows instructions carefully.\n",
      "\n",
      "Which is a species of fish? Tope or Rope\n",
      "Okay, I understand. Let's do this! \n",
      "\n",
      "**Please provide me with a list of the species of fish.**\n",
      "\n",
      "\n",
      "===== Streamed Sample #2 =====\n",
      "User prompt:\n",
      " Why can camels survive for long without water?\n",
      "\n",
      "### Model response ###:\n",
      " user\n",
      "You are a helpful assistant that follows instructions carefully.\n",
      "\n",
      "Why can camels survive for long without water?\n",
      "**\n",
      "**\n",
      "Okay, here's a detailed explanation of why camels can survive for a surprisingly long time without water:\n",
      "\n",
      "**The Key to Their Survival: A Remarkable Adaptation**\n",
      "\n",
      "Camels have evolved a truly remarkable physiological adaptation that allows them to survive for extended periods without drinking water. It's not about *not* needing water, but about a highly efficient and complex system. Here's a breakdown of the key factors:\n",
      "\n",
      "1. **Highly Efficient Kidneys:**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-create the stream (iterators are single-pass)\n",
    "streamed = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    ")\n",
    "\n",
    "for i, row in enumerate(islice(streamed, 3)):  # increase this number as desired\n",
    "    messages = row_to_messages(row)\n",
    "    # For inference, stop at the user turn (omit gold assistant response)\n",
    "    messages = messages[:2]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.9\n",
    "        )\n",
    "\n",
    "    print(f\"\\n===== Streamed Sample #{i} =====\")\n",
    "    print(\"User prompt:\\n\", messages[-1][\"content\"][:500])\n",
    "    print(\n",
    "        \"\\n### Model response ###:\\n\",\n",
    "        tokenizer.decode(outputs[0], skip_special_tokens=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5622a68",
   "metadata": {},
   "source": [
    "### Optional: PyTorch IterableDataset for Batched Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a5af59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 0 with shapes: {'input_ids': (4, 141), 'attention_mask': (4, 141)}\n",
      "Processed batch 1 with shapes: {'input_ids': (4, 256), 'attention_mask': (4, 256)}\n",
      "Processed batch 2 with shapes: {'input_ids': (4, 256), 'attention_mask': (4, 256)}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class ChatStreamDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self, hf_builder, tokenizer, device, max_length=1024, include_answer=False\n",
    "    ):\n",
    "        self.hf_builder = hf_builder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.include_answer = include_answer\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Fresh stream per worker/iterator\n",
    "        stream = self.hf_builder()\n",
    "        for row in stream:\n",
    "            messages = row_to_messages(row)\n",
    "            if not self.include_answer:\n",
    "                messages = messages[:2]  # [system, user]\n",
    "\n",
    "            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "            batch_inputs = self.tokenizer(\n",
    "                prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_length\n",
    "            )\n",
    "            # Move to device lazily\n",
    "            batch_inputs = {k: v.to(self.device) for k, v in batch_inputs.items()}\n",
    "            yield batch_inputs\n",
    "\n",
    "\n",
    "def make_dolly_stream():\n",
    "    return load_dataset(\n",
    "        \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    "    )\n",
    "\n",
    "\n",
    "iterable_ds = ChatStreamDataset(\n",
    "    hf_builder=make_dolly_stream,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=256,\n",
    "    include_answer=False,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"][0] for item in batch]\n",
    "    attn_mask = [item[\"attention_mask\"][0] for item in batch]\n",
    "    padded = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attn_mask},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {k: v.to(device) for k, v in padded.items()}\n",
    "\n",
    "\n",
    "loader = DataLoader(iterable_ds, batch_size=4, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "for bi, batch in enumerate(loader):\n",
    "    with torch.no_grad():\n",
    "        _ = model(**batch)\n",
    "    print(\n",
    "        f\"Processed batch {bi} with shapes:\",\n",
    "        {k: tuple(v.shape) for k, v in batch.items()},\n",
    "    )\n",
    "    if bi >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f252dc8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Finished:** 2025-08-22 12:56 UTC\n",
    "\n",
    "**Next steps:**\n",
    "- Try different datasets (e.g., `OpenAssistant/oasst1`) and adapt `row_to_messages`.\n",
    "- Experiment with system prompts to shift tone and style.\n",
    "- Integrate the streaming `IterableDataset` with training loops (TRL or custom).\n",
    "\n",
    "Happy building! 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
