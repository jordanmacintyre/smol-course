{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf31846",
   "metadata": {},
   "source": [
    "\n",
    "# Hands-On Tutorial: Chat Templates with SmolLM2 **(+ Streaming Datasets)**\n",
    "\n",
    "This end-to-end notebook teaches you how to:\n",
    "1. Load **SmolLM2-135M** and enable **chat templates** with `trl.setup_chat_format`.\n",
    "2. Generate text **with and without** templates to see the difference.\n",
    "3. Run **multi-turn conversations** using a structured `messages` format.\n",
    "4. **Stream datasets** from the Hugging Face Hub (`datasets.load_dataset(..., streaming=True)`), convert rows into chat messages, and **tokenize on the fly**.\n",
    "5. (Optional) Wrap a stream into a **PyTorch `IterableDataset`** for batched inference/eval.\n",
    "\n",
    "> Tip: You can run this on CPU/GPU. GPU is recommended for speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68112b",
   "metadata": {},
   "source": [
    "## 1) Setup & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on a fresh environment (e.g., Colab), uncomment:\n",
    "# !pip install -q transformers datasets trl huggingface_hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login()  # Uncomment and run to authenticate with your HF token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdadad3",
   "metadata": {},
   "source": [
    "## 2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import setup_chat_format\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27abb1",
   "metadata": {},
   "source": [
    "## 3) Device, Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf208066",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\n",
    "        \"mps\"\n",
    "        if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Enable chat template support (adds/aligns special tokens and template behavior)\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce5c77",
   "metadata": {},
   "source": [
    "## 4) What Are Chat Templates? Format a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85853dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that explains things clearly.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain the importance of chat templates in language models.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(\"Formatted prompt preview:\\n\")\n",
    "print(formatted_prompt[:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706f5b6",
   "metadata": {},
   "source": [
    "## 5) Generate with a Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, max_new_tokens=150, do_sample=True, temperature=0.7, top_p=0.9\n",
    "    )\n",
    "\n",
    "print(\"=== Model Output (with template) ===\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977cb2f3",
   "metadata": {},
   "source": [
    "## 6) Compare: Without a Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71847db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prompt = \"Explain the importance of chat templates in language models.\"\n",
    "raw_inputs = tokenizer(raw_prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    raw_outputs = model.generate(\n",
    "        **raw_inputs, max_new_tokens=150, do_sample=True, temperature=0.7, top_p=0.9\n",
    "    )\n",
    "\n",
    "print(\"=== Model Output (without template) ===\\n\")\n",
    "print(tokenizer.decode(raw_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506283c9",
   "metadata": {},
   "source": [
    "## 7) Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a witty assistant that answers concisely.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is quantum computing?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Quantum computing uses quantum bits (qubits) that can be in multiple states at once.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Explain it like I'm 5.\"},\n",
    "]\n",
    "\n",
    "mt_prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "mt_inputs = tokenizer(mt_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mt_outputs = model.generate(\n",
    "        **mt_inputs, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9\n",
    "    )\n",
    "\n",
    "print(\"=== Multi-Turn Output ===\\n\")\n",
    "print(tokenizer.decode(mt_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57658af",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Why Templates Matter (Quick Notes)\n",
    "\n",
    "- **Role-aware structure** helps the model separate instructions from answers.\n",
    "- **Consistency** across training/inference improves output stability.\n",
    "- **Fair evaluation**: using templates standardizes inputs across different models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db0a65",
   "metadata": {},
   "source": [
    "## 9) Stream a Dataset from the Hugging Face Hub (No Full Download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# Stream a dataset (no full download). You can swap for other datasets.\n",
    "streamed = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    ")\n",
    "\n",
    "# Peek a few rows to understand the schema\n",
    "for i, row in enumerate(islice(streamed, 3)):\n",
    "    print(f\"[{i}] keys:\", list(row.keys()))\n",
    "    print(\"instruction:\", row.get(\"instruction\"))\n",
    "    print(\"context:\", row.get(\"context\"))\n",
    "    print(\"response:\", (row.get(\"response\") or \"\")[:120], \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c928e",
   "metadata": {},
   "source": [
    "### Helper: Convert Streamed Rows â†’ Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f50b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_messages(row):\n",
    "    sys_msg = \"You are a helpful assistant that follows instructions carefully.\"\n",
    "    instr = row.get(\"instruction\") or \"\"\n",
    "    ctx = row.get(\"context\") or \"\"\n",
    "    user_msg = instr if not ctx else f\"{instr}\\n\\nContext:\\n{ctx}\"\n",
    "    asst_msg = row.get(\"response\") or \"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "    ]\n",
    "\n",
    "    # Include gold answer for inspection; for inference we usually omit it\n",
    "    if asst_msg.strip():\n",
    "        messages.append({\"role\": \"assistant\", \"content\": asst_msg})\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "# Smoke test on a couple of samples\n",
    "streamed = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    ")\n",
    "for i, row in enumerate(islice(streamed, 2)):\n",
    "    msgs = row_to_messages(row)\n",
    "    formatted = tokenizer.apply_chat_template(msgs, tokenize=False)\n",
    "    print(f\"---- Sample {i} (formatted preview) ----\\n{formatted[:400]} ...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec763ab",
   "metadata": {},
   "source": [
    "### On-the-Fly Tokenization & Inference Over the Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27461fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the stream (iterators are single-pass)\n",
    "streamed = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    ")\n",
    "\n",
    "for i, row in enumerate(islice(streamed, 3)):  # increase this number as desired\n",
    "    messages = row_to_messages(row)\n",
    "    # For inference, stop at the user turn (omit gold assistant response)\n",
    "    messages = messages[:2]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9\n",
    "        )\n",
    "\n",
    "    print(f\"\\n===== Streamed Sample #{i} =====\")\n",
    "    print(\"User prompt:\\n\", messages[-1][\"content\"][:500])\n",
    "    print(\"\\nModel response:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5622a68",
   "metadata": {},
   "source": [
    "### Optional: PyTorch IterableDataset for Batched Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5af59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "class ChatStreamDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self, hf_builder, tokenizer, device, max_length=1024, include_answer=False\n",
    "    ):\n",
    "        self.hf_builder = hf_builder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.include_answer = include_answer\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Fresh stream per worker/iterator\n",
    "        stream = self.hf_builder()\n",
    "        for row in stream:\n",
    "            messages = row_to_messages(row)\n",
    "            if not self.include_answer:\n",
    "                messages = messages[:2]  # [system, user]\n",
    "\n",
    "            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "            batch_inputs = self.tokenizer(\n",
    "                prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_length\n",
    "            )\n",
    "            # Move to device lazily\n",
    "            batch_inputs = {k: v.to(self.device) for k, v in batch_inputs.items()}\n",
    "            yield batch_inputs\n",
    "\n",
    "\n",
    "def make_dolly_stream():\n",
    "    return load_dataset(\n",
    "        \"databricks/databricks-dolly-15k\", split=\"train\", streaming=True\n",
    "    )\n",
    "\n",
    "\n",
    "iterable_ds = ChatStreamDataset(\n",
    "    hf_builder=make_dolly_stream,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_length=1024,\n",
    "    include_answer=False,\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"][0] for item in batch]\n",
    "    attn_mask = [item[\"attention_mask\"][0] for item in batch]\n",
    "    padded = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attn_mask},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {k: v.to(device) for k, v in padded.items()}\n",
    "\n",
    "\n",
    "loader = DataLoader(iterable_ds, batch_size=4, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "for bi, batch in enumerate(loader):\n",
    "    with torch.no_grad():\n",
    "        _ = model(**batch)\n",
    "    print(\n",
    "        f\"Processed batch {bi} with shapes:\",\n",
    "        {k: tuple(v.shape) for k, v in batch.items()},\n",
    "    )\n",
    "    if bi >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f252dc8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Finished:** 2025-08-22 12:56 UTC\n",
    "\n",
    "**Next steps:**\n",
    "- Try different datasets (e.g., `OpenAssistant/oasst1`) and adapt `row_to_messages`.\n",
    "- Experiment with system prompts to shift tone and style.\n",
    "- Integrate the streaming `IterableDataset` with training loops (TRL or custom).\n",
    "\n",
    "Happy building! ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
